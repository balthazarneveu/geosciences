{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model quantization investigations\n",
    "\n",
    "[quantization.py](quantization.py) provides a few functions used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantization import quantize_weights, get_array_size_in_bytes, quantize_model_per_layer\n",
    "from model_quantization import quantize_model\n",
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 201 # Pick up a model to quantize\n",
    "model = load_model(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.cat([p.flatten() for p in model.parameters() if p.requires_grad])\n",
    "params = params.detach().cpu().numpy()\n",
    "print(len(params), \"=\", model.count_parameters(), \"->\", get_array_size_in_bytes(params), \"Bytes\")\n",
    "quantize_model_per_layer(model, num_bits=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global weights distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(params, bins=1000)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('log count')\n",
    "plt.xlabel('parameter value')\n",
    "plt.grid()\n",
    "plt.title('Parameter distribution before quantization - all layers mixed')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need  for per-layer quantization\n",
    "The following graph shows that if we perform global model quantization (same scaling for all weights, we will loose a lot of precision as each layer's weight have a slightly different dynamic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "tot = len([1 for _ in model.named_parameters()])\n",
    "plt.figure(figsize=(10, tot//2*5))\n",
    "for idx, (name, param) in enumerate(model.named_parameters()):\n",
    "    if 'bias' not in name:\n",
    "        plt.subplot(tot//2, 2, idx//2 * 2 + 1)\n",
    "    else:\n",
    "        plt.subplot(tot//2, 2, idx//2 * 2 + 2)\n",
    "    if param.requires_grad:\n",
    "        layer_params = param.detach().cpu().numpy().flatten()\n",
    "        plt.hist(layer_params, bins=100, density=True, alpha=1, label=name)\n",
    "        # plt.title(f'Histogram for Layer: {name}')\n",
    "        plt.title('Histogram for weight' if 'weight' in name else 'Histogram for bias')\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('log count')\n",
    "        plt.xlabel('parameter value')\n",
    "        plt.legend()\n",
    "        plt.xlim(-1., 1.)\n",
    "        plt.grid()\n",
    "plt.suptitle('Histogram of weights and biases for each layer')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
