{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model quantization investigations\n",
    "\n",
    "[quantization.py](quantization.py) provides a few functions used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantization import get_array_size_in_bytes, dequantize_weights, dequantize_weights_per_layer\n",
    "from model_quantization import quantize_model_per_layer\n",
    "from infer import load_model\n",
    "import torch\n",
    "from math import sqrt, ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from metrics import compute_metrics\n",
    "from tqdm import tqdm\n",
    "from shared import (\n",
    "    ACCURACY, PRECISION, RECALL, F1_SCORE, IOU,\n",
    "    VALIDATION, TEST, TRAIN,\n",
    "    DEVICE\n",
    ")\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "device = DEVICE\n",
    "from evaluate import evaluate_model, evaluate_test_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 201 # Pick up a model to quantize\n",
    "exp = 204\n",
    "exp = 403\n",
    "exp = 702\n",
    "exp = 711\n",
    "num_bits=8 # If you want to change the number of bits, you need to reload the model\n",
    "# you can use 3 bits , it still works kind of correctly\n",
    "model, dl_dict, model_config = load_model(exp)\n",
    "original_model, _, _ = load_model(exp, get_data_loaders_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled_dict = evaluate_test_mode(original_model, dl_dict, save_path=Path(f'__submission_{exp:04d}_dataset_update.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(original_model, dl_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.cat([p.flatten() for p in model.parameters() if p.requires_grad])\n",
    "params = params.detach().cpu().numpy()\n",
    "print(len(params), \"=\", model.count_parameters(), \"->\", get_array_size_in_bytes(params), \"Bytes\")\n",
    "quantized_weights, quantization_parameters = quantize_model_per_layer(model, num_bits=num_bits)\n",
    "params_dequant = dequantize_weights_per_layer(quantized_weights, quantization_parameters)\n",
    "\n",
    "# Reinject dequantized weights into the model\n",
    "for name, param in model.named_parameters():\n",
    "    if name in params_dequant:\n",
    "        param.data = torch.nn.Parameter(torch.from_numpy(params_dequant[name])).to(device=device)\n",
    "        print(name, \"has been updated with quantized weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantized model\n",
    "evaluate_model(model, dl_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_key= \"conv_in_modality.conv_h.weight\"\n",
    "layer_key = list(original_model.named_parameters())[0][0]\n",
    "model_params_dict = dict(original_model.named_parameters())\n",
    "params_no_qant = model_params_dict[layer_key].flatten().detach().cpu().numpy()\n",
    "# Back to the original weights\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(params_no_qant, label=\"Original\")\n",
    "plt.plot(params_dequant[layer_key].flatten(), \".\", label=\"Dequantized\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(f\"Quantization result {layer_key}\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.hist(params_no_qant-params_dequant[layer_key].flatten(), bins=100, label=\"Error\")\n",
    "plt.legend()\n",
    "plt.title(f\"Quantization Error {layer_key}\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(dl_dict[VALIDATION]))\n",
    "with torch.no_grad():\n",
    "    output = original_model(img)\n",
    "    output_with_qant = model(img)\n",
    "selected_index = 12 # image to pick from the first validation batch\n",
    "\n",
    "\n",
    "plt.figsize = (10, 10)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(torch.sigmoid(output[selected_index, 0, ...]).detach().cpu().numpy())\n",
    "plt.title(f'Probability prediction \\noriginal model = {exp}')\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(torch.sigmoid(output_with_qant[selected_index, 0, ...]).detach().cpu().numpy())\n",
    "plt.title(f\"Output with quantized weights on {num_bits} bits\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title(\"Input\")\n",
    "plt.imshow(img[selected_index, 0, ...].detach().cpu().numpy().astype(float), cmap=\"gray\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title(\"Ground truth\")\n",
    "plt.imshow(label[selected_index, 0, ...].detach().cpu().numpy().astype(float))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Ground truth\")\n",
    "plt.imshow(label[selected_index, 0, ...].detach().cpu().numpy().astype(float))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figsize = (10, 10)\n",
    "plt.imshow(torch.sigmoid(output[selected_index, 0, ...]).detach().cpu().numpy()>0.5)\n",
    "plt.title(\"Binary Prediction mask\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = torch.sigmoid(output_with_qant)-torch.sigmoid(output)\n",
    "error = error[:, 0, ...].detach().cpu().numpy()\n",
    "plt.figure(figsize=(20, 20))\n",
    "n = int(sqrt(error.shape[0])+0.5)\n",
    "for idx in range(error.shape[0]):\n",
    "    plt.subplot(n, ceil(error.shape[0]/n), idx+1)\n",
    "    plt.title(f\"{np.abs(error)[idx].mean():.2%}\")\n",
    "    plt.imshow(error[idx])\n",
    "plt.suptitle(f\"Probability difference due to quantization error - {num_bits} bits\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global weights distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_params = torch.cat([p.flatten() for p in original_model.parameters() if p.requires_grad])\n",
    "original_params = original_params.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(original_params, bins=1000)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('log count')\n",
    "plt.xlabel('parameter value')\n",
    "plt.grid()\n",
    "plt.title('Parameter distribution before quantization - all layers mixed')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need  for per-layer quantization\n",
    "The following graph shows that if we perform global model quantization (same scaling for all weights, we will loose a lot of precision as each layer's weight have a slightly different dynamic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "tot = len([1 for _ in original_model.named_parameters()])\n",
    "plt.figure(figsize=(10, tot//2*5))\n",
    "for idx, (name, param) in enumerate(original_model.named_parameters()):\n",
    "    if 'bias' not in name:\n",
    "        plt.subplot(tot//2, 2, idx//2 * 2 + 1)\n",
    "    else:\n",
    "        plt.subplot(tot//2, 2, idx//2 * 2 + 2)\n",
    "    if param.requires_grad:\n",
    "        layer_params = param.detach().cpu().numpy().flatten()\n",
    "        plt.hist(layer_params, bins=100, density=True, alpha=1, label=name)\n",
    "        # plt.title(f'Histogram for Layer: {name}')\n",
    "        plt.title('Histogram for weight' if 'weight' in name else 'Histogram for bias')\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('log count')\n",
    "        plt.xlabel('parameter value')\n",
    "        plt.legend()\n",
    "        plt.xlim(-1., 1.)\n",
    "        plt.grid()\n",
    "plt.suptitle('Histogram of weights and biases for each layer')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
