{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "- [Balthazar Neveu](https://www.linkedin.com/in/balthazarneveu/)\n",
    "- Project= updated TP-5 for [Introduction to geosciences](https://www.master-mva.com/cours/introduction-a-lapprentissage-statistique-pour-les-geosciences/) | ENS Paris Saclay - [Master MVA](https://www.master-mva.com/) 2024\n",
    "- [Web version](https://balthazarneveu.github.io/geosciences) | [Github](https://github.com/balthazarneveu/geosciences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset exploration\n",
    "Pairs of patches and annotations (areas to segment). size 36x36 gray level.\n",
    "\n",
    "- Train set: 7211 patches from 12 wells.\n",
    "- Validation set: 2463 patches from 3 wells (majority in well 13) \n",
    "\n",
    "⚠️ WARNING no wells in common between validation and training ⚠️. \n",
    "\n",
    "🆕 REBOOT\n",
    "\n",
    "__Observations__\n",
    "- At first sight, the regions we're trying to segment look like thin dark lines.\n",
    "- Sometimes the positive areas are spread on both sides of the image (due to the circular nature of the well images).\n",
    "- Two images containing NaN values `validation/images/well_15_patch_201.npy` and `well_15_patch_202.npy` are discarded in the dataloader (sanity check before loading the images).\n",
    "\n",
    "![](figures/dataset_samples.png)\n",
    "\n",
    "## Dataloader\n",
    "[data_loader.py](data_loader.py) loads pairs of image, labels.\n",
    "A list of augmentations is provided in [augmentations.py](augmentations.py):\n",
    "- Horizontal roll (since the pipelines are circular)\n",
    "- Vertical/Horizontal flips can be performed randomly.\n",
    "Random augmentations (and shuffles) are only performed on the training set, validation set is frozen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🆕 Revision\n",
    "A reboot has been performed compared to the TP-5 version. Sections indicated with 🆕 show the main changes. \n",
    "\n",
    "Several critical issues have been corrected:\n",
    "- ❌ Problem : Metrics reduction bug\n",
    "- ✔️ Solution: simple unit tests, code review.\n",
    "- ❌ Validation has not intersection with Training set. *(=not the same wells)*! Not a good idea to monitor training with this. \n",
    "- ✔️ Solution: shuffle dataset (next level would be cross validation)\n",
    "- ❌ Problem: Potential other bugs? - More generally, how to aleviate the doubts I had about the training method and architectures.\n",
    "- ✔️ I created a toy example to validate that everything works correctly   \n",
    "\n",
    "\n",
    "### Metrics reduction: \n",
    "- I corrected a critical bug in metrics computation (reduction must be done over batches after computing dice score/IOU/precision/recall per images) . A [unitary pytest](test_metrics.py) has been added.\n",
    "- Special case of dice score when there is no corrosion (label image = 0). If the prediction is perfect, the original formulation of the dice score gives 0%. I corrected so it is 100% to avoid penalizing. Please note that this workaround breaks the loss \"continuity\"... (any tiny mispredicted pixel will bring back the dice score to nearly 0%).\n",
    "- I also checked that metrics don't change when batch size changes...\n",
    "### Train/Validation gap and monitoring\n",
    "\n",
    "\n",
    "> ⚠️ Although the initial idea of a validation set with wells not contained in the training set seems pretty good. It is a **very** deceitful idea when we're using the validation set as a metric to monitor training:\n",
    "> - pick the best model on validation dice score ... may simply get you an overall bad model but by chance good on the 3 wells of the validation set.\n",
    "> - LR scheduler based on this metric will reduce learning rate\n",
    "\n",
    "Usually, we use the validation metric to monitor overfitting etc... here the validation set is way more like a \"generalization to unseen data\" test set.\n",
    "\n",
    "Here's what happened, I'd naturally pick the \"best\" model based on the validation dice score.\n",
    "\n",
    "| Performances on the \"best model\" according to the validation dice score | Performances at the last epoch  |\n",
    "|:---: | :---: |\n",
    "| Train set Dice 66.2% | Train set Dice 76.5% |\n",
    "| ![](figures/dice_per_well_best_model_711.png) | ![](figures/dice_per_well_last_model_711.png) |\n",
    "| Validation Dice 82.3% | Validation Dice 81.1% |\n",
    "| ![](figures/dice_per_well_best_model_711_VALID.png) | ![](figures/dice_per_well_last_model_711_VALID.png) | \n",
    "\n",
    "----\n",
    "| Original train + validation sets wells distribution | New split distribution |\n",
    "|:---:| :---: |\n",
    "|![](figures/original_train_valid_distribution.png) | ![](figures/new_train_val_split.png) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation training validation\n",
    "After spotting the bug in the metrics computations, I decided to check the whole pipelines for bug.\n",
    "- Added [unit tests](test_metrics.py) on metrics\n",
    "- Created a toy example (with various of complexity) to validate the whole training process when labels are perfect.\n",
    "\n",
    "The toy example allows to check:\n",
    "- That the training loop works properly\n",
    "- That my models can segment perfectly labeled data properly\n",
    "- To check that models can be trained using the dice loss.\n",
    "\n",
    "\n",
    "|  | |\n",
    "|:---:| :---:|\n",
    "|![](figures/trivial_toy_example.png)| ![](figures/toy_trivial.gif) |\n",
    "\n",
    "\n",
    "<!-- |  | |\n",
    "|:---:| :---:|\n",
    "|![](figures/trivial_training_curves.png) | ![](figures/trivial_training_curves_BCE.png) | -->\n",
    "\n",
    "- Exp 621 (purple) is trained using weighted BCE (x2 in favor of positive examples) and batches of size 32\n",
    "- Exp 622 (green) is trained in the same conditions as 621 but with batches of size 256 (*which shows a slightly slower convergence*)\n",
    "- Exp 623 (red) is trained using dice loss ... we can see that there's a drop in DICE metric at epoch 23 (but the BCE decreases).  \n",
    "\n",
    "\n",
    "|  |\n",
    "|:---:|\n",
    "| ![](figures/trivial_training_curves_comparison.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures\n",
    "Three families of models are coded in different python files, access [model.py](model.py) to find your way\n",
    "All models are pretty flexible. \n",
    "What can be changed:\n",
    "- convolution sizes\n",
    "- number of layers\n",
    "- number of channels (hidden dimensions)\n",
    "- activation function (Relu of Leaky ReLu were tested)\n",
    "- number of input channels (ready to apply to other modalities)\n",
    "- number of output channels (ready for multi classes).\n",
    "Since all models inherit from `BaseModel`, the number of parameters and the receptive field can easily be retrieved.\n",
    "Please note that all models do not include the sigmoïd applied to the final layer: we take logits as outputs and the loss function or further inference is in charge of applying the sigmoïd to convert these to probabilities.\n",
    "\n",
    "\n",
    "| Model name | Number of parameters  | Convolution sizes | Number of layers | Activation | Receptive field (H, V) |\n",
    "| :---:| :---:| :---:|  :---:|  :---:| :---:|\n",
    "| Vanilla Stacked convolutions| 260k |  $(3,3)$ | 5 | ReLu | $(11,11)$ |\n",
    "| Stack convolution | 1.904M | $3^{\\circ} \\perp 5$ | 5 | LeakyReLu | $(11,21)$ |\n",
    "|Legacy U-Net|  775k | $(3,3)$ | 3 scales | LeakyReLu | $(27, 27)$ |\n",
    "| 🆕 Flexible U-Net Teacher | 6.4M | $(3^{\\circ},3)$ | 3 scales with 4-2-1 conv blocks | Leaky Relu |  $(81,81)$  |\n",
    "| 🆕 Flexible U-Net Student | 576k | $(3^{\\circ},3)$ | 3 scales with 2-1-1 conv blocks | Leaky Relu |  $(73,73)$  |\n",
    "\n",
    "> Please note that the receptive field of the Flexible UNets are larger than the image, we avoid adding too much convolutions at the top level/bottleneck to avoid issues at boundaries and we use repeat padding to let the network think that vertically, the well goes continuously (instead of zero-ing out). For the azimuth direction, a special padding has been designed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Vanilla convolution stack](model_vanillaconv.py)  (single scale)\n",
    "\n",
    "*Remark: I coded the stacked convolution before I re-discovered the slide on the proposed vanilla model (\"baseline\" model from the slides).*\n",
    "\n",
    "Provided as a \"baseline\" model, uses ReLu .\n",
    "\n",
    "| Proposed diagram by the SLB challenge organizers | Netron visualization |\n",
    "|:---:| :---: |\n",
    "| ![](figures/vanilla_convolution.png) | ![](figures/netron_10_vanilla.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Stacked convolution](model_stackedconv.py) (single scale)\n",
    "- Flexible design with a parameterized amount of layers\n",
    "- Base convolution block is a separable convolution directionwise ($H \\perp V$)\n",
    "  - The horizontal convolution pads using the \"circular\" convolution option which allows dealing with the specificity of dwell images.\n",
    "  - The vertical convolution pads by repeating gray levels.\n",
    "  - This should explain the notation  $3^{\\circ} \\perp 5$\n",
    "- Input modality convolution block allows going from 1 to `h_dim` channels.\n",
    "- Output modality convolution block allows going from `h_dim` channels back to a single channel.\n",
    "- Last layer outputs an image of the same size as the original one. Since we use the BCE Loss with Logits at first, the output of the network are logits (*not probabilities*), Sigmoid is not included.\n",
    "- Possibility to use residual connections when the number of layers is a multiple of 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Legacy \"Classic\" UNet](model_unet.py)\n",
    "- 3 scales.\n",
    "  - $(36, 36) \\rightarrow (18, 18) \\rightarrow (9, 9)$\n",
    "  - Downsample by decimating information (skip 1 pixel over 4)\n",
    "  - Upsample with a bilinear interpolation.\n",
    "  - Concatenate skip connections together. \n",
    "- Large receptive field (27,27)\n",
    "\n",
    "![](figures/classic_unet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🆕 [Flexible UNET](model_circular_unet.py)\n",
    "\n",
    "After the \"reboot\", I decided to recode a new UNET:\n",
    "- pre-pad the inputs (circular padding on the azimuth direction) and crop at the end.\n",
    "- extend the channels dimension by a factor of 2 when downsampling, shrink the channels by 2 when upsampling.\n",
    "- Allow to specify:\n",
    "  - the amount of convolution blocks per scale for both encoder and decoder (and bottleneck)\n",
    "  - the thickness (width) used at the first scale... the rest is deduced progressively (*using the rule x2 when downsampling by 2*) \n",
    "\n",
    "During the padding phase, we repeat a few pixels at the top and bottom (to go to a size of 40 instead of 36 which allows adding a potential 4th scale - this was not retained as the best model though and allows increasing the receptive field).\n",
    "| Padding mechanism for the flexible circular UNet |\n",
    "|:---: |\n",
    "| ![](figures/circular_padding.png) |\n",
    "\n",
    "Syntax for the encoder / bottleneck / decoder goes as follows: `[4 , 2, 1], 1, [1, 2, 4 ]`  and the convolution block Thickness. It allows easily playing with the architecture.\n",
    "\n",
    "\n",
    "|Exp id  | Dice Validation | Encoder | Bottlneck | Decoder | Thickness  | # params | diagram |\n",
    "|:---:|:---:| :---:    |:---:      | :---:    |:---:      |:---:     |:---:    |\n",
    "| 53   | 79% | [4, 2, 1]   | 1 | [1, 2, 4]|  64 | 6.4M params |  ![](figures/netron_unet_53.png) |\n",
    "| 2001 | 76%  | [4, 2, 1] | 1 | [1, 2, 4] | 16 | 576k params |![](figures/netron_unet_1004png.png) |\n",
    "|52 | 71% | [4, 2] | 1 | [2, 4] | 16 | 219k params | ![](figures/netron_exp52_miniUnet_2layers.png) |\n",
    "\n",
    "Note that the architecture defined in exp 2001 (with 576k parameters) performances will be boosted to 79% dice score to when trained with distillation  (exp 1004).\n",
    "\n",
    "| The more the merrier... more parameters and larger architectures lead to better performances |\n",
    "|:---:|\n",
    "![](figures/unet_comparisons.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Taining\n",
    "### Defining experiments\n",
    "- An experiment is defined by as specific ID (like 300) and the whole configuration is versioned under git in the [experiments_definition.py](experiments_definition.py) file:\n",
    "    - architecture (model name, number of layers, convolution sizes)\n",
    "    - augmentations\n",
    "    - 🆕 loss (BCE, BCE+Dice, BCE with more weights to the positive samples).\n",
    "    - hyper parameters\n",
    "    - 🆕  distillation parameters (teacher is just the ID of a previous experiment, temperature, weight between distillation loss and label loss)\n",
    "- Tracking is performed using [Weights and Biases](https://wandb.ai/balthazarneveu/geosciences-segmentation/workspace?workspace=user-balthazarneveu)\n",
    "\n",
    "\n",
    "### Infrastructure\n",
    "- It is possible to train locally with a Nvidia very tiny GPU T500 with 4Gb of RAM.\n",
    "  - `python TP_5/train.py -e 300 301`\n",
    "  - `-nowb` allows disabling logging to weights and biases for quick prototyping\n",
    "  - `-e` to specify a list of experiments.\n",
    "- The same experiment can be trained on a remote server `python TP_5/remote_training.py -e 300 301 -u kaggle_username -p` \n",
    "- To be able to train on the remote servers of Kaggle with 16Gb of RAM, I customized a remote training template that I wrote ([MVA-Pepites](https://github.com/balthazarneveu/mva_pepites)). I hosted the [dataset](https://www.kaggle.com/datasets/balthazarneveu/mva-geosciences-segmentation-dataset-slb) under Kaggle.\n",
    "It is possible to train several experiments sequentially. More details here : [remote training](https://github.com/balthazarneveu/mva_pepites/blob/main/illustrations/overview.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring\n",
    "I implemented a set of [metrics](metrics.py) on the validation set:\n",
    "- Accuracy (does not mean much because if the network returns all zeros, the accuracy is around 89%).\n",
    "- Precision , Recall. Recall seems interesting allows having a metric of how well we detected positive areas.\n",
    "- Segmentation specific metrics : Dice loss (also named F1-score) to measure the balance between precision and recall.\n",
    "- IoU (intersection over union).\n",
    "\n",
    "We train the network using BCE loss (with logits). The problem is casted as a per-pixel binary classification (background = 0, foreground = 1). Since the background class is over represented, we can weight the positive class a bit more.\n",
    "The [loss.py](loss.py) file shows the possibilities.\n",
    "\n",
    "- The best model selection is performed on the accuracy criterion (legacy)\n",
    "- The learning rate plateau decision is performed on the validation loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training curves analyzis\n",
    "Let's see how the dice metric evolves for various experiments (different architectures or hyperparmeters). We systematically have decreasing BCE losses on the training set but stops decreasing or increases on the validation set. This may be explained as:\n",
    "- an overfitting phenomenon despite efforts to implement relevant augmentations \n",
    "- or the network trying to be efficient for all wells data and being less efficient on the wells dedicated to validation. *To verify this hypothesis, we'd need to filter metrics by well which requires trickier code*. \n",
    "\n",
    "![](figures/train_validation_losses.png)\n",
    "\n",
    "\n",
    "\n",
    "Beware that the BCE loss may not reflect the performances for segmentation, which is why we also monitor validation metrics during training.\n",
    "\n",
    "![](figures/metrics_iou_dice.png)\n",
    "\n",
    "-----\n",
    "\n",
    "# Best model\n",
    "Best dice metric was achieved with **experiment 201** (in brown), `dice=83.7%` . In this experiment, we use:\n",
    "- Stacked Convolution Network 1.904M (5 layers, `h_dim=256`, conv $3^{\\circ} \\perp 5$ , LeakyReLu, receptive field $(11,21)$.\n",
    "- Adam Optimizer Learning rate: $10^{-4}$, plateau (patience 5 epochs, LR decrease factor : $0.8$)\n",
    "- Loss: Weighted BCE with a weight of 2 on the positive examples.\n",
    "- Augmentation: Horizontal shifts, vertical and horizontal flips.\n",
    "- Batch size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Results visualization\n",
    "\n",
    "### Batch inference\n",
    "\n",
    "`python TP_5/infer.py -e 201 -m validation -o TP_5/pretrained_models`\n",
    "\n",
    "![](figures/inference_validation.png)\n",
    "\n",
    "### Interactive visualization\n",
    "To be able to visualize results, it is possible to perform live inference and compare several models.\n",
    "Inference is performed live on the GPU\n",
    "\n",
    "\n",
    "|![](figures/interactive_demo_browse.gif)| ![](figures/interactive_demo_compare_models.gif) | ![](figures/interactive_demo_test_mode.gif) |\n",
    "|:---:|:---:|:---:|\n",
    "| We can browse between images using the left / right arrow  | page up/ page down to switch between models | test mode without label |\n",
    "\n",
    "\n",
    "`python TP_5/interactive_inference.py -i \"TP_5/data/train/images/well_2*.npy\" -e 201 402 300 -m TP_5/pretrained_models --gui qt --preload`\n",
    "\n",
    "- Using the right regexp, in the `-i` argument, you can select training, validation or test images.\n",
    "- Pretrained models (201 Stacked convolutions, 300 Vanilla convolutions , 402 UNET) are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results analyzis\n",
    "\n",
    "### Color code\n",
    "- Pixels flagged in black or green: correctly labeled (black=background, green=foreground)\n",
    "- Pixels flagged in red: predicted background (0), groundtruth = foreground (1) *a.k.a False Negative*\n",
    "- Pixels flagged in blue: predicted foreground (1), groundtruth = background (0) *a.k.a False Positive*\n",
    "### Coherence with shift and additive noise\n",
    "- Our stacked convolution network which uses horizontal convolutions with circular wrapping is able to segment corrosion areas which are located at the image boundary.\n",
    "- Segmentation results are almost unaffected by additive noise\n",
    "\n",
    "|![](figures/interactive_demo_shift.gif) | ![](figures/interactive_demo_noise.gif) |\n",
    "|:---:|:---:|\n",
    "| Slider allow to shift the input horizontally |  Slider allows to add a bit of noise|\n",
    "\n",
    "### Labeling relevance\n",
    "\n",
    "Trying to reach best accuracy may be in vain. As a matter of fact, it seems that sometimes the labels are less relevant than the network prediction.\n",
    "\n",
    "|![](figures/annotations_accuracy.png) | ![](figures/annotations_accuracy_2.png) |\n",
    "|:---:| :----: |\n",
    "| Label mis-location | Label is too thick. Network prediction is more thin and better located. The corrosion line is 2 pixels wide , not 3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations and potential improvements\n",
    "\n",
    "#### Potential improvements\n",
    "\n",
    "Here are some ideas for improvements:\n",
    "- It seems like a lot of labeled segmentated areas are single continuous lines...but our predictions are sometimes broken in several pieces... \n",
    "  - Even the U-Net with its large receptive field has not been able to fully cope with this property. Performing extra downsamples by **maxpooling over the vertical dimension** only and then repeating the information when upsampling could allow to share the information at all scales. In practice, we could add more scales to the U-Net but only in the vertical dimension.\n",
    "  - Using **attention mechanism** may allow sharing information across the whole image.\n",
    "\n",
    "| ![](figures/broken_line_UNET.png) | ![](figures/broken_line_Stacked_conv.png) |\n",
    "|:--:|:---:|\n",
    "|UNet predicts 2 thin areas of corrosion which is wrong. Also nottice the top and bottom rows which are almost always wrong, this may come from the padding issue | Stacked convolution network saves the day - but let's not make it a generality and the predicted result does not really match the expected mask anyway|\n",
    "- Hard Negative Mining: Once we get a first network, we could mine the difficult examples (evaluate the performances over all patches and sort the most difficult ones). In a second step, we can re-train the pretrained network on these hard patches.\n",
    "\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "Due to the time spent on this lab session, here are some points **I didn't have time to implement**.\n",
    "- \"Dice Loss Bug\": For some reason, I could not minimize the dice loss (1-dice coefficient). Don't know whether the loss is wrong or not.\n",
    "- Extra augmentation by a bit of additive white gaussian noise, blur or sharpening, multiply the signals to slightly augment the dynamic range, add S curves to augment (increase/decrease contrast).\n",
    "- perform an evaluation of the metrics per well (there may be more difficult wells).\n",
    "- try the so called Focal Loss which is said to adapt the weight of the BCE kind of automatically.\n",
    "- cross validation (take several split of the train, validation set and evaluate the average and standard deviation of the results for 1 experiment configuration).\n",
    "\n",
    "\n",
    "\n",
    "#### True difficulties\n",
    "- Very difficult to assess the quality of annotations. A toy example would be very nice (segment darker lines over various backgrounds).\n",
    "- Not sure whether or not the validation starts increasing because of overfitting or basically that the validation set does not have the same distribution as the training set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the model lightweight\n",
    "### Quantization\n",
    "\n",
    "### Weights quantization\n",
    "\n",
    "[Auxiliary notebook on quantization](model_quantization.html) | code: [model_quantization.ipynb](model_quantization.ipynb)\n",
    "\n",
    "\n",
    "🪶 We'll use model `201` at first which only weights 7.26Mb on disk as storing only 1.9M parameters.\n",
    "- We can quantify the weights to any precision we'd like 16bits or 8bits. \n",
    "- But we can try 12 or 4bits which would require a dedicated packing algorithm.\n",
    "\n",
    "> Weight quantization purpose is just to compress storage space (not RAM) at the cost of decompressing the weights. Not that lossless zip-like compression could be applied on top of that).\n",
    "\n",
    "#### Remarks\n",
    "\n",
    "Below is the global distribution of convolution weights of the model, no distinction made between layers.\n",
    "\n",
    "![](figures/weights_global_distrib.png)\n",
    "\n",
    "By analyzing the distribution of weights at each layer, it is clear that each layer's weight has its own range of values which is why I picked to quantify per layer.\n",
    "\n",
    "By compressing to signed 8bit integer weights, we're able to shrink the model by a factor of 4.\n",
    "If we take a look at the probability prediction errors with the 8bit compressed model, we have a less than an average 0.1% error and visually the segmentations prediction with the quantized weights don't look to far away from the original model.\n",
    "We can check the first validation batch (32 images).\n",
    "\n",
    "![](figures/quant_8bits.png)\n",
    "\n",
    "\n",
    "We need to lead more thorough validation and compute validation metrics over the whole validation set.\n",
    "\n",
    "\n",
    "| Quantization | None | 16bits | 8bits | 4bits |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| Size on disk | 7.26Mb |  3.6Mb | 1.8Mb | ~1Mb |\n",
    "| Format | Float32 | int16 | int8 | packed 1 bit sign, 3bits content |\n",
    "| Dice Score | 83.7%  | 83.7% | 83.6%| 84.4% |\n",
    "| IoU |  72.47% | 72.47% | 72.43% | 73.55% |\n",
    "\n",
    "\n",
    "<!-- Original -->\n",
    "<!-- \n",
    "{'accuracy': 0.9683680534362793,\n",
    " 'precision': 0.7998656034469604,\n",
    " 'recall': 0.8879660964012146,\n",
    " 'dice': 0.8370986580848694,\n",
    " 'iou': 0.7247620820999146} -->\n",
    "\n",
    "\n",
    "<!-- Quantized 16b -->\n",
    "<!-- {'accuracy': 0.9683681130409241,\n",
    " 'precision': 0.7998725771903992,\n",
    " 'recall': 0.8879573941230774,\n",
    " 'dice': 0.8370987772941589,\n",
    " 'iou': 0.7247614860534668} -->\n",
    "\n",
    "<!-- Quantized 8b -->\n",
    " <!-- {'accuracy': 0.9683061838150024,\n",
    " 'precision': 0.7994349598884583,\n",
    " 'recall': 0.8878950476646423,\n",
    " 'dice': 0.8368082642555237,\n",
    " 'iou': 0.7243764996528625} -->\n",
    "\n",
    "\n",
    "<!-- Quantized 3b -->\n",
    "<!-- \n",
    " {'accuracy': 0.9725220203399658,\n",
    " 'precision': 0.8826404213905334,\n",
    " 'recall': 0.8197945952415466,\n",
    " 'dice': 0.8448435068130493,\n",
    " 'iou': 0.7355394959449768} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
