{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "- [Balthazar Neveu](https://www.linkedin.com/in/balthazarneveu/)\n",
    "- Project= updated TP-5 for [Introduction to geosciences](https://www.master-mva.com/cours/introduction-a-lapprentissage-statistique-pour-les-geosciences/) | ENS Paris Saclay - [Master MVA](https://www.master-mva.com/) 2024\n",
    "- [Web version](https://balthazarneveu.github.io/geosciences) | [Github](https://github.com/balthazarneveu/geosciences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "> 🆕 since I discovered some issues (mentioned later) compared to the original TP-5 version , I had to start all over again ...\n",
    "> \n",
    "> I modified this technical report with some updates and mentioned it by adding the logo 🆕. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset exploration\n",
    "Pairs of patches and annotations (areas to segment). size 36x36 gray level.\n",
    "\n",
    "- Train set: 7211 patches from 12 wells.\n",
    "- Validation set: 2463 patches from 3 wells (majority in well 13) \n",
    "\n",
    "⚠️ WARNING no wells in common between validation and training ⚠️. \n",
    "\n",
    "\n",
    "__Observations__\n",
    "- At first sight, the regions we're trying to segment look like thin dark lines.\n",
    "- Sometimes the positive areas are spread on both sides of the image (due to the circular nature of the well images).\n",
    "- Two images containing NaN values `validation/images/well_15_patch_201.npy` and `well_15_patch_202.npy` are discarded in the dataloader (sanity check before loading the images).\n",
    "\n",
    "![](figures/dataset_samples.png)\n",
    "\n",
    "## Dataloader\n",
    "[data_loader.py](data_loader.py) loads pairs of image, labels.\n",
    "A list of augmentations is provided in [augmentations.py](augmentations.py):\n",
    "- Horizontal roll (since the pipelines are circular)\n",
    "- Vertical/Horizontal flips can be performed randomly.\n",
    "Random augmentations (and shuffles) are only performed on the training set, validation set is frozen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🆕 Revision\n",
    "A reboot has been performed compared to the TP-5 version. Sections indicated with 🆕 show the main changes. \n",
    "\n",
    "Several critical issues have been corrected:\n",
    "- ❌ Problem : Metrics reduction bug\n",
    "- ✔️ Solution: simple unit tests, code review.\n",
    "- ❌ Validation has not intersection with Training set. *(=not the same wells)*! Not a good idea to monitor training with this. \n",
    "- ✔️ Solution: shuffle dataset (next level would be cross validation)\n",
    "- ❌ Problem: Potential other bugs? - More generally, how to aleviate the doubts I had about the training method and architectures.\n",
    "- ✔️ I created a toy example to validate that everything works correctly   \n",
    "\n",
    "\n",
    "### Metrics reduction: \n",
    "- I corrected a critical bug in metrics computation (reduction must be done over batches after computing dice score/IOU/precision/recall per images) . A [unitary pytest](test_metrics.py) has been added.\n",
    "- Special case of dice score when there is no corrosion (label image = 0). If the prediction is perfect, the original formulation of the dice score gives 0%. I corrected so it is 100% to avoid penalizing. Please note that this workaround breaks the loss \"continuity\"... (any tiny mispredicted pixel will bring back the dice score to nearly 0%).\n",
    "- I also checked that metrics don't change when batch size changes...\n",
    "### Train/Validation gap and monitoring\n",
    "\n",
    "\n",
    "> ⚠️ Although the initial idea of a validation set with wells not contained in the training set seems pretty good. It is a **very** deceitful idea when we're using the validation set as a metric to monitor training:\n",
    "> - pick the best model on validation dice score ... may simply get you an overall bad model but by chance good on the 3 wells of the validation set.\n",
    "> - LR scheduler based on this metric will reduce learning rate\n",
    "\n",
    "Usually, we use the validation metric to monitor overfitting etc... here the validation set is way more like a \"generalization to unseen data\" test set.\n",
    "\n",
    "Here's what happened, I'd naturally pick the \"best\" model based on the validation dice score.\n",
    "\n",
    "| Performances on the \"best model\" according to the validation dice score | Performances at the last epoch  |\n",
    "|:---: | :---: |\n",
    "| Train set Dice 66.2% | Train set Dice 76.5% |\n",
    "| ![](figures/dice_per_well_best_model_711.png) | ![](figures/dice_per_well_last_model_711.png) |\n",
    "| Validation Dice 82.3% | Validation Dice 81.1% |\n",
    "| ![](figures/dice_per_well_best_model_711_VALID.png) | ![](figures/dice_per_well_last_model_711_VALID.png) | \n",
    "\n",
    "----\n",
    "| Original train + validation sets wells distribution | New split distribution |\n",
    "|:---:| :---: |\n",
    "|![](figures/original_train_valid_distribution.png) | ![](figures/new_train_val_split.png) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🆕 Segmentation training sanity check \n",
    "After spotting the bug in the metrics computations, I decided to check the whole pipelines for bug.\n",
    "- Added [unit tests](test_metrics.py) on metrics\n",
    "- Created a toy example (with various of complexity) to validate the whole training process when labels are perfect.\n",
    "\n",
    "The toy example allows to check:\n",
    "- That the training loop works properly\n",
    "- That my models can segment perfectly labeled data properly\n",
    "- To check that models can be trained using the dice loss.\n",
    "\n",
    "\n",
    "|  | |\n",
    "|:---:| :---:|\n",
    "|![](figures/trivial_toy_example.png)| ![](figures/toy_trivial.gif) |\n",
    "\n",
    "\n",
    "<!-- |  | |\n",
    "|:---:| :---:|\n",
    "|![](figures/trivial_training_curves.png) | ![](figures/trivial_training_curves_BCE.png) | -->\n",
    "\n",
    "- Exp 621 (purple) is trained using weighted BCE (x2 in favor of positive examples) and batches of size 32\n",
    "- Exp 622 (green) is trained in the same conditions as 621 but with batches of size 256 (*which shows a slightly slower convergence*)\n",
    "- Exp 623 (red) is trained using dice loss ... we can see that there's a drop in DICE metric at epoch 23 (but the BCE decreases).  \n",
    "\n",
    "\n",
    "|  |\n",
    "|:---:|\n",
    "| ![](figures/trivial_training_curves_comparison.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures\n",
    "Three families of models are coded in different python files, access [model.py](model.py) to find your way\n",
    "All models are pretty flexible. \n",
    "What can be changed:\n",
    "- convolution sizes\n",
    "- number of layers\n",
    "- number of channels (hidden dimensions)\n",
    "- activation function (Relu of Leaky ReLu were tested)\n",
    "- number of input channels (ready to apply to other modalities)\n",
    "- number of output channels (ready for multi classes).\n",
    "Since all models inherit from `BaseModel`, the number of parameters and the receptive field can easily be retrieved.\n",
    "Please note that all models do not include the sigmoïd applied to the final layer: we take logits as outputs and the loss function or further inference is in charge of applying the sigmoïd to convert these to probabilities.\n",
    "\n",
    "\n",
    "| Model name | Number of parameters  | Convolution sizes | Number of layers | Activation | Receptive field (H, V) |\n",
    "| :---:| :---:| :---:|  :---:|  :---:| :---:|\n",
    "| Vanilla Stacked convolutions| 260k |  $(3,3)$ | 5 | ReLu | $(11,11)$ |\n",
    "| Stack convolution | 1.904M | $3^{\\circ} \\perp 5$ | 5 | LeakyReLu | $(11,21)$ |\n",
    "|Legacy U-Net|  775k | $(3,3)$ | 3 scales | LeakyReLu | $(27, 27)$ |\n",
    "| 🆕 Flexible U-Net Teacher | 6.4M | $(3^{\\circ},3)$ | 3 scales with 4-2-1 conv blocks | Leaky Relu |  $(81,81)$  |\n",
    "| 🆕 Flexible U-Net Student | 576k | $(3^{\\circ},3)$ | 3 scales with 2-1-1 conv blocks | Leaky Relu |  $(73,73)$  |\n",
    "\n",
    "> Please note that the receptive field of the Flexible UNets are larger than the image, we avoid adding too much convolutions at the top level/bottleneck to avoid issues at boundaries and we use repeat padding to let the network think that vertically, the well goes continuously (instead of zero-ing out). For the azimuth direction, a special padding has been designed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Vanilla convolution stack](model_vanillaconv.py)  (single scale)\n",
    "\n",
    "*Remark: I coded the stacked convolution before I re-discovered the slide on the proposed vanilla model (\"baseline\" model from the slides).*\n",
    "\n",
    "Provided as a \"baseline\" model, uses ReLu .\n",
    "\n",
    "| Proposed diagram by the SLB challenge organizers | Netron visualization |\n",
    "|:---:| :---: |\n",
    "| ![](figures/vanilla_convolution.png) | ![](figures/netron_10_vanilla.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Stacked convolution](model_stackedconv.py) (single scale)\n",
    "- Flexible design with a parameterized amount of layers\n",
    "- Base convolution block is a separable convolution directionwise ($H \\perp V$)\n",
    "  - The horizontal convolution pads using the \"circular\" convolution option which allows dealing with the specificity of dwell images.\n",
    "  - The vertical convolution pads by repeating gray levels.\n",
    "  - This should explain the notation  $3^{\\circ} \\perp 5$\n",
    "- Input modality convolution block allows going from 1 to `h_dim` channels.\n",
    "- Output modality convolution block allows going from `h_dim` channels back to a single channel.\n",
    "- Last layer outputs an image of the same size as the original one. Since we use the BCE Loss with Logits at first, the output of the network are logits (*not probabilities*), Sigmoid is not included.\n",
    "- Possibility to use residual connections when the number of layers is a multiple of 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Legacy \"Classic\" UNet](model_unet.py)\n",
    "- 3 scales.\n",
    "  - $(36, 36) \\rightarrow (18, 18) \\rightarrow (9, 9)$\n",
    "  - Downsample by decimating information (skip 1 pixel over 4)\n",
    "  - Upsample with a bilinear interpolation.\n",
    "  - Concatenate skip connections together. \n",
    "- Large receptive field (27,27)\n",
    "\n",
    "![](figures/classic_unet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🆕 [Flexible UNET](model_circular_unet.py)\n",
    "\n",
    "After the \"reboot\", I decided to recode a new UNET:\n",
    "- pre-pad the inputs (circular padding on the azimuth direction) and crop at the end.\n",
    "- extend the channels dimension by a factor of 2 when downsampling, shrink the channels by 2 when upsampling.\n",
    "- Allow to specify:\n",
    "  - the amount of convolution blocks per scale for both encoder and decoder (and bottleneck)\n",
    "  - the thickness (width) used at the first scale... the rest is deduced progressively (*using the rule x2 when downsampling by 2*) \n",
    "\n",
    "During the padding phase, we repeat a few pixels at the top and bottom (to go to a size of 40 instead of 36 which allows adding a potential 4th scale - this was not retained as the best model though and allows increasing the receptive field).\n",
    "| Padding mechanism for the flexible circular UNet |\n",
    "|:---: |\n",
    "| ![](figures/circular_padding.png) |\n",
    "\n",
    "Syntax for the encoder / bottleneck / decoder goes as follows: `[4 , 2, 1], 1, [1, 2, 4 ]`  and the convolution block Thickness. It allows easily playing with the architecture.\n",
    "\n",
    "\n",
    "|Exp id  | Dice Validation | Encoder | Bottlneck | Decoder | Thickness  | # params | diagram |\n",
    "|:---:|:---:| :---:    |:---:      | :---:    |:---:      |:---:     |:---:    |\n",
    "| 53   | 79% | [4, 2, 1]   | 1 | [1, 2, 4]|  64 | 6.4M params |  ![](figures/netron_unet_53.png) |\n",
    "| 2001 | 76%  | [4, 2, 1] | 1 | [1, 2, 4] | 16 | 576k params |![](figures/netron_unet_1004png.png) |\n",
    "|52 | 71% | [4, 2] | 1 | [2, 4] | 16 | 219k params | ![](figures/netron_exp52_miniUnet_2layers.png) |\n",
    "\n",
    "Note that the architecture defined in exp 2001 (with 576k parameters) performances will be boosted to 79% dice score to when trained with distillation  (exp 1004).\n",
    "\n",
    "\n",
    "| The more the merrier... more parameters and larger architectures lead to better performances |\n",
    "|:---:|\n",
    "| ![](figures/unet_comparisons.png)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Training\n",
    "\n",
    "### Defining experiments\n",
    "- An experiment is defined by as specific ID (like 300) and the whole configuration is versioned under git in the [experiments_definition.py](experiments_definition.py) file:\n",
    "    - architecture (model name, number of layers, convolution sizes)\n",
    "    - augmentations\n",
    "    - loss (BCE, 🆕  BCE+Dice, BCE with more weights to the positive samples).\n",
    "    - hyper parameters\n",
    "    - distillation parameters (teacher is just the ID of a previous experiment, temperature, weight between distillation loss and label loss) 🆕  \n",
    "- Tracking is performed using **[Weights and Biases](https://wandb.ai/balthazarneveu/corrosion-segmentation) see the dashboard here**\n",
    "\n",
    "\n",
    "![](figures/wandb_tracking.png)\n",
    "\n",
    "### Infrastructure\n",
    "- It is possible to train some experiments locally with a Nvidia very tiny GPU T500 with 4Gb of RAM.\n",
    "  - `python TP_5/train.py -e 53 10`\n",
    "  - `-nowb` allows disabling logging to weights and biases for quick prototyping\n",
    "  - `-e` to specify a list of experiments.\n",
    "- The same experiment can be trained on a remote server `python TP_5/remote_training.py -e 53 10 -u kaggle_username -p` \n",
    "- To be able to train on the remote servers of Kaggle with 16Gb of RAM, I customized a remote training template that I wrote ([MVA-Pepites](https://github.com/balthazarneveu/mva_pepites)). I hosted the [dataset](https://www.kaggle.com/datasets/balthazarneveu/mva-geosciences-segmentation-dataset-slb) under Kaggle.\n",
    "It is possible to train several experiments sequentially. More details here : [remote training](https://github.com/balthazarneveu/mva_pepites/blob/main/illustrations/overview.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring\n",
    "I implemented a set of [metrics](metrics.py) on the validation set:\n",
    "- Accuracy (does not mean much because if the network returns all zeros, the accuracy is around 89%).\n",
    "- Precision , Recall. Recall seems interesting allows having a metric of how well we detected positive areas.\n",
    "- Segmentation specific metrics : Dice loss (also named F1-score) to measure the balance between precision and recall.\n",
    "- IoU (intersection over union).\n",
    "\n",
    "🆕  We train the network using ~~BCE loss (with logits)~~ a combination of the **Dice loss and BCE loss.**\n",
    "\n",
    "The problem is casted as a per-pixel binary classification (background = 0, foreground = 1). Since the background class is over represented, we can weight the positive class a bit more.\n",
    "The [loss.py](loss.py) file shows the possibilities.\n",
    "\n",
    "- The best model selection is performed on the accuracy criterion (legacy)\n",
    "- The learning rate plateau decision is performed on the validation loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training curves analyzis\n",
    "\n",
    "#### Searching for the best architecture and hyper parameters\n",
    "\n",
    "Below we exhibit all tests which have been performed (various archicture, sizes and several experiment to check hyper parameters)\n",
    "\n",
    "\n",
    "| Dice validation score |\n",
    "| :----: |\n",
    "| ![](figures/models_training_comparison_curves.png) |\n",
    "| Experiments configurations summary |\n",
    "| ![](figures/models_training_comparison.png) |\n",
    "\n",
    "\n",
    "\n",
    "# Best model (teacher)\n",
    "Experiment 53 .\n",
    "- Flexible UNET\n",
    "- 3 scales, encoder [4, 2, 1] - bottleneck 1 - decoder [1, 2, 4]. Convolution block thickness 16, Leaky Relu.\n",
    "- Circular padding\n",
    "- 6.4M parameters.\n",
    "- Adam Optimizer Learning rate LR $5 10^{-4}$  plateau (patience 10 epochs, LR decrease factor : $0.8$)\n",
    "- Stopped after 100 epochs before models start to overfit.\n",
    "- Augmentation: Horizontal shifts, vertical and horizontal flips.\n",
    "- Batch size 128\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "#### Overfitting for UNet with more than 4 million parameters.\n",
    "\n",
    "Can we push this model further?\n",
    "\n",
    "If you let the \"big\" networks train longer, we nottice that these end up overfitting (in the sense that training loss decreases and validation increases... while the dice loss we monitor remains kind of constant.)\n",
    "\n",
    "![](figures/overfitting.png)\n",
    "\n",
    "The spotted overfitting truly affects the generalization capability of the model, making it go from 62% score to 58% on the challenge test set.\n",
    "\n",
    "Note: I didn't have time to implement extra augmentations such as\n",
    "- exposure variation\n",
    "- S-curves\n",
    "- gaussian noise addition\n",
    "- geometric shear or tiny vertical rescales.\n",
    "\n",
    "But I'm not sure that these extra augmentations or weight decay would be enough to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Results visualization\n",
    "\n",
    "### Batch inference\n",
    "\n",
    "`python TP_5/infer.py -e 1004 -m validation -o TP_5/__pretrained_models`\n",
    "\n",
    "![](figures/inference_validation.png)\n",
    "\n",
    "### Interactive visualization\n",
    "To be able to visualize results, it is possible to perform live inference and compare several models.\n",
    "Inference is performed live on the GPU\n",
    "\n",
    "\n",
    "|![](figures/interactive_demo_browse.gif)| ![](figures/interactive_demo_compare_models.gif) | ![](figures/interactive_demo_test_mode.gif) |\n",
    "|:---:|:---:|:---:|\n",
    "| We can browse between images using the left / right arrow  | page up/ page down to switch between models | test mode without label |\n",
    "\n",
    "\n",
    "`python TP_5/interactive_inference.py -i \"TP_5/data/train/images/well_2*.npy\" -e 53 1004 -m TP_5/pretrained_models --gui qt --preload`\n",
    "\n",
    "- Using the right regexp, in the `-i` argument, you can select training, validation or test images.\n",
    "- Pretrained models (53 UNet6.4k, 1004 UNet 576k parameters) are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results analyzis\n",
    "\n",
    "![](figures/final_gui_interface_2.png)\n",
    "\n",
    "### Color code\n",
    "- Pixels flagged in black or green: correctly labeled (black=background, green=foreground)\n",
    "- Pixels flagged in red: predicted background (0), groundtruth = foreground (1) *a.k.a False Negative*\n",
    "- Pixels flagged in blue: predicted foreground (1), groundtruth = background (0) *a.k.a False Positive*\n",
    "### Coherence with shift and additive noise\n",
    "- Our stacked convolution network which uses horizontal convolutions with circular wrapping is able to segment corrosion areas which are located at the image boundary.\n",
    "- Segmentation results are almost unaffected by additive noise\n",
    "\n",
    "|![](figures/interactive_demo_shift.gif) | ![](figures/interactive_demo_noise.gif) |\n",
    "|:---:|:---:|\n",
    "| Slider allow to shift the input horizontally |  Slider allows to add a bit of noise|\n",
    "\n",
    "### Labeling relevance\n",
    "\n",
    "Trying to reach best accuracy may be in vain. As a matter of fact, it seems that sometimes the labels are less relevant than the network prediction.\n",
    "\n",
    "|![](figures/annotations_accuracy.png) | ![](figures/annotations_accuracy_2.png) |\n",
    "|:---:| :----: |\n",
    "| Label mis-location | Label is too thick. Network prediction is more thin and better located. The corrosion line is 2 pixels wide , not 3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the model lightweight\n",
    "> Feel free to read the **[Weights and biases report on compressing the model](https://wandb.ai/balthazarneveu/corrosion-segmentation/reports/Corrosion-segmentation--Vmlldzo3MTQ1MTAy)**\n",
    "\n",
    "The following figures shows that performances remain equivalent with a huge reduction of model size and storage footprint (25MB teacher -> 550kb student + 8bits quantization).\n",
    "\n",
    "![](figures/summary_validation_set.png)\n",
    "\n",
    "### Distillation\n",
    "🔎 [Weights and biases report section on distillation](https://wandb.ai/balthazarneveu/corrosion-segmentation/reports/Corrosion-segmentation--Vmlldzo3MTQ1MTAy#quantization)\n",
    "\n",
    "Distillation has been added to the main training without the need to rewrite the whole code. \n",
    "We distilled the best model we got `53` UNET 6.4M parameters into a smaller 564k model (`1004`) which achieves similar performances.\n",
    "\n",
    "\n",
    "### Weights quantization\n",
    "\n",
    "🔎 [Weights and biases report section on quantization](https://wandb.ai/balthazarneveu/corrosion-segmentation/reports/Corrosion-segmentation--Vmlldzo3MTQ1MTAy#quantization)\n",
    "\n",
    "[Auxiliary notebook on quantization](model_quantization.html) | code: [model_quantization.ipynb](model_quantization.ipynb)\n",
    "\n",
    "\n",
    "🪶 We'll use distilled model `1004` (UNET 564k) at first which weights 2.2Mb on disk as storing only 1.9M parameters.\n",
    "- We can quantify the weights to any precision we'd like 16bits or 8bits. \n",
    "- But we can try 12 or 4bits which would require a dedicated packing algorithm.\n",
    "\n",
    "> Weight quantization purpose is just to compress storage space (not RAM) at the cost of decompressing the weights. Not that lossless zip-like compression could be applied on top of that).\n",
    "\n",
    "#### Remarks\n",
    "\n",
    "Below is the global distribution of convolution weights of the model, no distinction made between layers.\n",
    "\n",
    "![](figures/model_1004_global_weights.png)\n",
    "\n",
    "By analyzing the distribution of weights at each layer, it is clear that each layer's weight has its own range of values which is why I picked to quantify per layer.\n",
    "\n",
    "By compressing to signed 8bit integer weights, we're able to shrink the model by a factor of 4.\n",
    "If we take a look at the probability prediction errors with the 8bit compressed model, we have a less than an average 0.1% error and visually the segmentations prediction with the quantized weights don't look to far away from the original model.\n",
    "We can check the first validation batch (128 images).\n",
    "\n",
    "![](figures/model_1004_quantization_8bits_error_on_proba.png)\n",
    "\n",
    "\n",
    "Here's the summary on performances of quantization of model 1004.\n",
    "\n",
    "\n",
    "| Quantization | None | 16bits | 8bits | 4bits |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| Size on disk | 2.2MB |  1.1MB | 550kB | $\\leq$ 300kB |\n",
    "| Format | Float32 | int16 | int8 | packed 1 bit of sign + 3bits content |\n",
    "| Dice Score | 78.937%  | 78.95% | 78.956%| 73.41% |\n",
    "| IoU |  71.549%| 71.57% | 71.584% | 65.15% |\n",
    "| Dice score **test set** | 66.24% | - | **66.38%** | - |\n",
    "\n",
    "\n",
    "<!-- Original -->\n",
    "<!-- \n",
    "{'accuracy': 0.9683680534362793,\n",
    " 'precision': 0.7998656034469604,\n",
    " 'recall': 0.8879660964012146,\n",
    " 'dice': 0.8370986580848694,\n",
    " 'iou': 0.7247620820999146} -->\n",
    "\n",
    "\n",
    "<!-- Quantized 16b -->\n",
    "<!-- {'accuracy': 0.9683681130409241,\n",
    " 'precision': 0.7998725771903992,\n",
    " 'recall': 0.8879573941230774,\n",
    " 'dice': 0.8370987772941589,\n",
    " 'iou': 0.7247614860534668} -->\n",
    "\n",
    "<!-- Quantized 8b -->\n",
    " <!-- {'accuracy': 0.9683061838150024,\n",
    " 'precision': 0.7994349598884583,\n",
    " 'recall': 0.8878950476646423,\n",
    " 'dice': 0.8368082642555237,\n",
    " 'iou': 0.7243764996528625} -->\n",
    "\n",
    "\n",
    "<!-- Quantized 3b -->\n",
    "<!-- \n",
    " {'accuracy': 0.9725220203399658,\n",
    " 'precision': 0.8826404213905334,\n",
    " 'recall': 0.8197945952415466,\n",
    " 'dice': 0.8448435068130493,\n",
    " 'iou': 0.7355394959449768} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This project was an interesting first contact with segmentation and model compression.\n",
    "- The design of a UNet tailored for the specificities of well imaging (circular azimuth, and very small 36x36) has allowed to achieve correct performances at segmenting corroded areas. \n",
    "- It is possible to shrink the segmentation model down to a point where it can be stored on 550kB (for instance as part of the firmware binary).\n",
    "- Generalization capabilities to unseen wells are probably not so great (as reflected by the score on the data challenge - I achieved a maximum of 66.24% score)... \n",
    "\n",
    "\n",
    "Test set score: 66.24 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations and potential improvements\n",
    "A lot of time during this project was spent on the \"reboot\" phase to find and correct a few issues mentioned at the begining. \n",
    "\n",
    "#### Potential improvements\n",
    "\n",
    "Here are some ideas for improvements:\n",
    "- One important point is that we only have access to a tiny vertical windows and I guess that the relevance could be much improved if we'd treat larger vertical windows (corrosion areas seem to often look a lot like dark lines). Taking decisions near the vertical boundaries of the 36x36 image is much harder when you don't have a clue what's above or below. \n",
    "- An **ensembling technique** could have been deployed (majority voting between several models)...to be later distilled into a smaller model.\n",
    "- Hard Negative Mining: Once we get a first network, we could mine the difficult examples (evaluate the performances over all patches and sort the most difficult ones). In a second step, we can re-train the pretrained network on these hard patches.\n",
    "- About the UNet architecture: Max pooling could have been used (instead of skipping) and stride convolutions instead of bilinear upsampling. Same goes with batch normalization.\n",
    "- A simple review of the predicted mask sometimes reveal a few isolated unlikely pixels... A few morphological operations like closing/opening could aleviate a few false positives.\n",
    "\n",
    "\n",
    "#### Missing elements\n",
    "\n",
    "Due to the time spent on the TP-5 lab session and project, here are some points **I didn't have time to implement**.\n",
    "- ~~\"Dice Loss Bug\": For some reason, I could not minimize the dice loss (1-dice coefficient). Don't know whether the loss is wrong or not.~~  -> fixed\n",
    "- Extra augmentation by a bit of additive white gaussian noise, blur or sharpening, multiply the signals to slightly augment the dynamic range, add S curves to augment (increase/decrease contrast).\n",
    "- ~~perform an evaluation of the metrics per well (there may be more difficult wells).~~\n",
    "- try the so called Focal Loss which is said to adapt the weight of the BCE kind of automatically.\n",
    "- cross validation (take several split of the train, validation set and evaluate the average and standard deviation of the results for 1 experiment configuration). Could have been interesting to check generalization capabilities to totally unseen wells.\n",
    "- (Not asked) effort to reduce the model memory footprint in RAM or execution time assessment\n",
    "- Label refinement: questioning the quality of the dataset and labels is important but improving it (either automatically or semi-automatically) is a huge task and moreover, we don't play a fair game here as **we only have 36x36 patches** (and sometimes corrosion is hard to see to the naked eye honnestly - i think the annotations have been made with a full vision of the well, coherence along the vertical axis may have helped annotators a lot).\n",
    "\n",
    "\n",
    "#### True difficulties\n",
    "- Very difficult to assess the quality of annotations. -> A toy example has been made to make sure everything was ok. \n",
    "- ~~Not sure whether or not the validation loss starts increasing because of overfitting or basically that the validation set does not have the same distribution as the training set!~~   -> Re-done dataset split."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
