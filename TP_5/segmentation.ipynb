{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "- [Balthazar Neveu](https://www.linkedin.com/in/balthazarneveu/)\n",
    "- TP-5 [Introduction to geosciences](https://www.master-mva.com/cours/introduction-a-lapprentissage-statistique-pour-les-geosciences/) | ENS Paris Saclay - [Master MVA](https://www.master-mva.com/) 2024\n",
    "- [Web version](https://balthazarneveu.github.io/geosciences) | [Github](https://github.com/balthazarneveu/geoscience)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset exploration\n",
    "Pairs of patches and annotations (areas to segment). size 36x36 gray level.\n",
    "\n",
    "- Train set: 7211 patches from 12 wells.\n",
    "- Validation set: 2463 patches from 3 wells (majority in well 13)\n",
    "\n",
    "__Observations__\n",
    "- At first sight, the regions we're trying to segment look like thin dark lines.\n",
    "- Sometimes the positive areas are spread on both sides of the image (due to the circular nature of the well images).\n",
    "- Two images containing NaN values `validation/images/well_15_patch_201.npy` and `well_15_patch_202.npy` are discarded in the dataloader (sanity check before loading the images).\n",
    "\n",
    "![](figures/dataset_samples.png)\n",
    "\n",
    "## Dataloader\n",
    "[data_loader.py](data_loader.py) loads pairs of image, labels.\n",
    "A list of augmentations is provided in [augmentations.py](augmentations.py):\n",
    "- Horizontal roll (since the pipelines are circular)\n",
    "- Vertical/Horizontal flips can be performed randomly.\n",
    "Random augmentations (and shuffles) are only performed on the training set, validation set is frozen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures\n",
    "Three families of models are coded in [model.py](model.py).\n",
    "All models are pretty flexible. \n",
    "What can be changed:\n",
    "- convolution sizes\n",
    "- number of layers\n",
    "- number of channels (hidden dimensions)\n",
    "- activation function (Relu of Leaky ReLu were tested)\n",
    "- number of input channels (ready to apply to other modalities)\n",
    "- number of output channels (ready for multi classes).\n",
    "Since all models inherit from `BaseModel`, the number of parameters and the receptive field can easily be retrieved.\n",
    "Please note that all models do not include the sigmoïd applied to the final layer: we take logits as outputs and the loss function or further inference is in charge of applying the sigmoïd to convert these to probabilities.\n",
    "\n",
    "\n",
    "| Model name | Number of parameters  | Convolution sizes | Number of layers | Activation | Receptive field (H, V) |\n",
    "| :---:| :---:| :---:|  :---:|  :---:| :---:|\n",
    "| Vanilla Stacked convolutions| 260k |  $(3,3)$ | 5 | ReLu | $(11,11)$ |\n",
    "| Stack convolution | 1.904M | $3^{\\circ} \\perp 5$ | 5 | LeakyReLu | $(11,21)$ |\n",
    "|U-Net|  775k | $(3,3)$ | 3 scales | LeakyReLu | $(27, 27)$ |\n",
    "\n",
    "\n",
    "#### Stacked convolution (single scale)\n",
    "- Flexible design with a parameterized amount of layers\n",
    "- Base convolution block is a separable convolution directionwise ($H \\perp V$)\n",
    "  - The horizontal convolution pads using the \"circular\" convolution option which allows dealing with the specificity of dwell images.\n",
    "  - The vertical convolution pads by repeating gray levels.\n",
    "  - This should explain the notation  $3^{\\circ} \\perp 5$\n",
    "- Input modality convolution block allows going from 1 to `h_dim` channels.\n",
    "- Output modality convolution block allows going from `h_dim` channels back to a single channel.\n",
    "- Last layer outputs an image of the same size as the original one. Since we use the BCE Loss with Logits at first, the output of the network are logits (*not probabilities*), Sigmoid is not included.\n",
    "- Possibility to use residual connections when the number of layers is a multiple of 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla convolution stack (single scale)\n",
    "\n",
    "*Remark: I coded the stacked convolution before I re-discovered the slide on the proposed vanilla model (\"baseline\" model from the slides).*\n",
    "\n",
    "Provided as a \"baseline\" model, uses ReLu .\n",
    "\n",
    "\n",
    "![](figures/vanilla_convolution.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNet\n",
    "- 3 scales.\n",
    "  - $(36, 36) \\rightarrow (18, 18) \\rightarrow (9, 9)$\n",
    "  - Downsample by decimating information (skip 1 pixel over 4)\n",
    "  - Upsample with a bilinear interpolation.\n",
    "  - Concatenate skip connections together. \n",
    "- Large receptive field (27,27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Taining\n",
    "### Defining experiments\n",
    "- An experiment is defined by as specific ID (like 300) and the whole configuration is versioned under git in the [experiments.py](experiments.py) file:\n",
    "    - architecture (model name, number of layers, convolution sizes)\n",
    "    - augmentations\n",
    "    - loss\n",
    "    - hyper parameters\n",
    "- Tracking is performed using [Weights and Biases](https://wandb.ai/balthazarneveu/geosciences-segmentation/workspace?workspace=user-balthazarneveu)\n",
    "\n",
    "\n",
    "### Infrastructure\n",
    "- It is possible to train locally with a Nvidia very tiny GPU T500 with 4Gb of RAM.\n",
    "  - `python TP_5/train.py -e 300 301`\n",
    "  - `-nowb` allows disabling logging to weights and biases for quick prototyping\n",
    "  - `-e` to specify a list of experiments.\n",
    "- The same experiment can be trained on a remote server `python TP_5/remote_training.py -e 300 301 -u kaggle_username -p` \n",
    "- To be able to train on the remote servers of Kaggle with 16Gb of RAM, I customized a remote training template that I wrote ([MVA-Pepites](https://github.com/balthazarneveu/mva_pepites)). I hosted the [dataset](https://www.kaggle.com/datasets/balthazarneveu/mva-geosciences-segmentation-dataset-slb) under Kaggle.\n",
    "It is possible to train several experiments sequentially. More details here : [remote training](https://github.com/balthazarneveu/mva_pepites/blob/main/illustrations/overview.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring\n",
    "I implemented a set of [metrics](metrics.py) on the validation set:\n",
    "- Accuracy (does not mean much because if the network returns all zeros, the accuracy is around 89%).\n",
    "- Precision , Recall. Recall seems interesting allows having a metric of how well we detected positive areas.\n",
    "- Segmentation specific metrics : Dice loss (also named F1-score) to measure the balance between precision and recall.\n",
    "- IoU (intersection over union).\n",
    "\n",
    "We train the network using BCE loss (with logits). The problem is casted as a per-pixel binary classification (background = 0, foreground = 1). Since the background class is over represented, we can weight the positive class a bit more.\n",
    "The [loss.py](loss.py) file shows the possibilities.\n",
    "\n",
    "- The best model selection is performed on the accuracy criterion (legacy)\n",
    "- The learning rate plateau decision is performed on the validation loss (BCE).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training curves analyzis\n",
    "Let's see how the dice metric evolves for various experiments (different architectures or hyperparmeters). We systematically have decreasing BCE losses on the training set but stops decreasing or increases on the validation set. This may be explained as:\n",
    "- an overfitting phenomenon despite efforts to implement relevant augmentations \n",
    "- or the network trying to be efficient for all wells data and being less efficient on the wells dedicated to validation. *To verify this hypothesis, we'd need to filter metrics by well which requires trickier code*. \n",
    "\n",
    "![](figures/train_validation_losses.png)\n",
    "\n",
    "\n",
    "\n",
    "Beware that the BCE loss may not reflect the performances for segmentation, which is why we also monitor validation metrics during training.\n",
    "\n",
    "![](figures/metrics_iou_dice.png)\n",
    "\n",
    "-----\n",
    "\n",
    "# Best model\n",
    "Best dice metric was achieved with **experiment 201** (in brown), `dice=83.7%` . In this experiment, we use:\n",
    "- Stacked Convolution Network 1.904M (5 layers, `h_dim=256`, conv $3^{\\circ} \\perp 5$ , LeakyReLu, receptive field $(11,21)$.\n",
    "- Adam Optimizer Learning rate: $10^{-4}$, plateau (patience 5 epochs, LR decrease factor : $0.8$)\n",
    "- Loss: Weighted BCE with a weight of 2 on the positive examples.\n",
    "- Augmentation: Horizontal shifts, vertical and horizontal flips.\n",
    "- Batch size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Results visualization\n",
    "\n",
    "### Batch inference\n",
    "\n",
    "`python TP_5/infer.py -e 201 -m validation -o TP_5/pretrained_models`\n",
    "\n",
    "![](figures/inference_validation.png)\n",
    "\n",
    "### Interactive visualization\n",
    "To be able to visualize results, it is possible to perform live inference and compare several models.\n",
    "Inference is performed live on the GPU\n",
    "\n",
    "\n",
    "|![](figures/interactive_demo_browse.gif)| ![](figures/interactive_demo_compare_models.gif) | ![](figures/interactive_demo_test_mode.gif) |\n",
    "|:---:|:---:|:---:|\n",
    "| We can browse between images using the left / right arrow  | page up/ page down to switch between models | test mode without label |\n",
    "\n",
    "\n",
    "`python TP_5/interactive_inference.py -i \"TP_5/data/train/images/well_2*.npy\" -e 201 402 300 -m TP_5/pretrained_models --gui qt --preload`\n",
    "\n",
    "- Using the right regexp, in the `-i` argument, you can select training, validation or test images.\n",
    "- Pretrained models (201 Stacked convolutions, 300 Vanilla convolutions , 402 UNET) are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results analyzis\n",
    "\n",
    "### Color code\n",
    "- Pixels flagged in black or green: correctly labeled (black=background, green=foreground)\n",
    "- Pixels flagged in red: predicted background (0), groundtruth = foreground (1) *a.k.a False Negative*\n",
    "- Pixels flagged in blue: predicted foreground (1), groundtruth = background (0) *a.k.a False Positive*\n",
    "### Coherence with shift and additive noise\n",
    "- Our stacked convolution network which uses horizontal convolutions with circular wrapping is able to segment corrosion areas which are located at the image boundary.\n",
    "- Segmentation results are almost unaffected by additive noise\n",
    "\n",
    "|![](figures/interactive_demo_shift.gif) | ![](figures/interactive_demo_noise.gif) |\n",
    "|:---:|:---:|\n",
    "| Slider allow to shift the input horizontally |  Slider allows to add a bit of noise|\n",
    "\n",
    "### Labeling relevance\n",
    "\n",
    "Trying to reach best accuracy may be in vain. As a matter of fact, it seems that sometimes the labels are less relevant than the network prediction.\n",
    "\n",
    "|![](figures/annotations_accuracy.png) | ![](figures/annotations_accuracy_2.png) |\n",
    "|:---:| :----: |\n",
    "| Label mis-location | Label is too thick. Network prediction is more thin and better located. The corrosion line is 2 pixels wide , not 3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations and potential improvements\n",
    "\n",
    "#### Potential improvements\n",
    "\n",
    "Here are some ideas for improvements:\n",
    "- It seems like a lot of labeled segmentated areas are single continuous lines...but our predictions are sometimes broken in several pieces... \n",
    "  - Even the U-Net with its large receptive field has not been able to fully cope with this property. Performing extra downsamples by **maxpooling over the vertical dimension** only and then repeating the information when upsampling could allow to share the information at all scales. In practice, we could add more scales to the U-Net but only in the vertical dimension.\n",
    "  - Using **attention mechanism** may allow sharing information across the whole image.\n",
    "\n",
    "| ![](figures/broken_line_UNET.png) | ![](figures/broken_line_Stacked_conv.png) |\n",
    "|:--:|:---:|\n",
    "|UNet predicts 2 thin areas of corrosion which is wrong. Also nottice the top and bottom rows which are almost always wrong, this may come from the padding issue | Stacked convolution network saves the day - but let's not make it a generality and the predicted result does not really match the expected mask anyway|\n",
    "- Hard Negative Mining: Once we get a first network, we could mine the difficult examples (evaluate the performances over all patches and sort the most difficult ones). In a second step, we can re-train the pretrained network on these hard patches.\n",
    "\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "Due to the time spent on this lab session, here are some points **I didn't have time to implement**.\n",
    "- \"Dice Loss Bug\": For some reason, I could not minimize the dice loss (1-dice coefficient). Don't know whether the loss is wrong or not.\n",
    "- Extra augmentation by a bit of additive white gaussian noise, blur or sharpening, multiply the signals to slightly augment the dynamic range, add S curves to augment (increase/decrease contrast).\n",
    "- perform an evaluation of the metrics per well (there may be more difficult wells).\n",
    "- try the so called Focal Loss which is said to adapt the weight of the BCE kind of automatically.\n",
    "- cross validation (take several split of the train, validation set and evaluate the average and standard deviation of the results for 1 experiment configuration).\n",
    "\n",
    "\n",
    "\n",
    "#### True difficulties\n",
    "- Very difficult to assess the quality of annotations. A toy example would be very nice (segment darker lines over various backgrounds).\n",
    "- Not sure whether or not the validation starts increasing because of overfitting or basically that the validation set does not have the same distribution as the training set!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
